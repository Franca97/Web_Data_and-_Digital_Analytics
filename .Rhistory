ggtitle("Donald Trump Sentiment Evolution")
dev.off()
View(tweets_df)
View(tweets)
View(tweets_unnested)
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
library(wordcloud)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
jpeg("TrumpWordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
tweets_df_trump <- tweets_df
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@NewsHour", n=2500, includeRts=T) # userTimline to get the n first tweets
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@NewsHour", n=2500, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@TheEllenShow", n=2500, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
tweets_df_Ellen <- tweets_df
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Ellen DeGeneres.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Ellen DeGeneres Sentiment Evolution")
dev.off()
jpeg("Ellen DeGeneres Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@USATODAYhealth", n=2500, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
tweets_df_USAToday <- tweets_df
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("USA Today Health.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("USA Today Health Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("USA Today Health Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#Country sentiment analysis
USA_df <- rbind(tweets_df_USAToday, tweets_df_Ellen, tweets_df_trump)
View(tweets_df_Ellen)
View(tweets_df_trump)
View(tweets_df_USAToday)
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500, includeRts=T) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
tweets_df_trump <- tweets_df
#Country sentiment analysis
USA_df <- rbind(tweets_df_USAToday, tweets_df_Ellen, tweets_df_trump)
#' Only keeping date after February 2020
USA_df$created <- as.Date(USA_df$created, "%d/%m/%Y") # add this column to the dataset
USA_df <- USA_df %>% filter(created >="2020-02-01")
USA_df$id <- seq(1, nrow(USA_df))
tweets_unnested <- unnest_tokens(USA_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
USA_df <- inner_join(USA_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, USA_df, mean)
#' Plot the average sentiment per date
jpeg("USA.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("USA Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("USA Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#'
#'  Boris Johnson Twitter Data
#'
tweets_bojo <- userTimeline("BorisJohnson", n=2500) # userTimline to get the n first tweets
tweets_df_bojo <- twListToDF(tweets_bojo) # convert it into a dataframe
#' Checking Twitter Data
tweets_df_bojo$created <- as.Date(tweets_df_bojo$created, "%d/%m/%Y") # add this column to the dataset
tweets_df_bojo <- tweets_df_bojo %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day_bojo <- count(tweets_df_bojo, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_bojo, aes(created, n)) + geom_point()
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
corpus_bojo <- Corpus(VectorSource(tweets_df_bojo$text)) # extract actual text and store it in a corpus
corpus_bojo
#' Pre-process the corpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub('â', "'", x))})
corpus_bojo <- tm_map(corpus_bojo, changeStrangeApostrophes)
#' Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
corpus_bojo <- tm_map(corpus_bojo, removeURL)
#' Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
corpus_bojo <- tm_map(corpus_bojo, remove, 'â')
corpus_bojo <- tm_map(corpus_bojo, remove, '-')
corpus_bojo <- tm_map(corpus_bojo, remove, 'â¬')
corpus_bojo <- tm_map(corpus_bojo, remove, "'")
corpus_bojo <- tm_map(corpus_bojo, remove, 'â')
corpus_bojo <- tm_map(corpus_bojo, remove, 'â')
corpus_bojo <- tm_map(corpus_bojo, remove, "/")
corpus_bojo <- tm_map(corpus_bojo, remove, "@")
corpus_bojo <- tm_map(corpus_bojo, remove, "\\|")
#' Remove Numbers
corpus_bojo <- tm_map(corpus_bojo, removeNumbers)
#' Remove punctuation
corpus_bojo <- tm_map(corpus_bojo, removePunctuation)
#' Stip whitespaces
corpus_bojo <- tm_map(corpus_bojo, stripWhitespace)
#' Text to lowercase
corpus_bojo <- tm_map(corpus_bojo, content_transformer(tolower))
#' Remove stopwords
corpus_bojo <-  tm_map(corpus_bojo, removeWords, stopwords("english"))
#' Stem the document
corpus_bojo <- tm_map(corpus_bojo, stemDocument, language = "english")
#' Checking with wordcloud
wordcloud(corpus_bojo, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
#' Checking with wordcloud
jpeg("Boris Johnson General Wordcloud.jpg", width = 1500)
wordcloud(corpus_bojo, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
dev.off()
dtm_bojo <- TermDocumentMatrix(corpus_bojo)
dtm_bojo
#' Assign some identifier to Document names in the DTM
dtm_bojo$dimnames$Docs <- tweets_df_bojo$text
#' Remove zero-entries from the DTM
dtm_bojo <- dtm_bojo[unique(dtm_bojo$i), ] # pick only non zero documents
dtm_bojo # has not changed anything?!
topic_number_bojo <- FindTopicsNumber(
dtm_bojo, #DTM object
topics = seq(from = 2, to = 15, by = 1), #Number of topics to test, i.e. between 2 and 15
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #Metrics to check
control = list(seed = 100), #Random seed
verbose = TRUE
)
#' Plot topic Models
#'
FindTopicsNumber_plot(topic_number_bojo)  # six or seven topics
#' Run an LDA model
bojo_tweets_LDA <- LDA(dtm_bojo, k = 5, control = list(seed = 100))
#' Get the Top 10 Terms for each topic
bojo_tidyLda <- tidy(bojo_tweets_LDA)
bojo_topTerms <- bojo_tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
bojo_topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title and y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
View(corpus_bojo)
tweets_df <- tweets_df_bojo
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Boris Johnson.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Boris Johnson Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("Boris Johnson Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@bbchealth", n=2500, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
tweets_df_BBC <- tweets_df
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("BBC Health.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("BBC Health News Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("BBC Health Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@LiamPayne", n=2500, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
tweets_df_Liam <- tweets_df
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Liam Payne.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Liam Payne Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("Liam Payne Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#Country sentiment analysis
USA_df <- rbind(tweets_df_bojo, tweets_df_Liam, tweets_df_BBC)
#' Only keeping date after February 2020
USA_df$created <- as.Date(USA_df$created, "%d/%m/%Y") # add this column to the dataset
USA_df <- USA_df %>% filter(created >="2020-02-01")
USA_df$id <- seq(1, nrow(USA_df))
tweets_unnested <- unnest_tokens(USA_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
USA_df <- inner_join(USA_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, USA_df, mean)
#' Plot the average sentiment per date
jpeg("UK.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("UK Sentiment Evolution")
dev.off()
#Wordcloud
library(reshape2) #used for datawrangling using acast() function
library(wordcloud)
jpeg("UK Wordcloud.jpg", width = 1500)
tweets_unnested %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_bojo, aes(created, n)) + geom_point()
View(tweets_bojo)
View(tweets_df_bojo)
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(tweets_df_bojo$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub('’', "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '—')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '€')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '“')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '”')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("english")) #when specifying function parameters, they are specified as arguments after the function
otherStopWords <- c("…", "ca", "your", "just", "even", "new", "use", "right", "one", "today", "time", "pick", "here", "will", "give", "otherwhise", "can", "here", "part", "look", "part") #other stopwords, they were defined after inspecting the results without them. You can try to execute this line, analyze the results and check for words that provide no meaning. You will find all of those.
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "english")
tweetsDTM <- DocumentTermMatrix(tweetsCorpus)
tweetsDTM
tweetsDTM <- tweetsDTM[unique(tweetsDTM$i), ] #pick only non zero-entry documents
tweetsDTM
topicNumberMetrics <- FindTopicsNumber(
tweetsDTM, #DTM object
topics = seq(from = 2, to = 15, by = 1), #Number of topics to test, i.e. between 2 and 15
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #Metrics to check
control = list(seed = 100), #Random seed
verbose = TRUE
)
FindTopicsNumber_plot(topicNumberMetrics)
tweetsLDA <- LDA(tweetsDTM, k = 5, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Topics Boris Johnson.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
