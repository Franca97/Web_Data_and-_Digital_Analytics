x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@maybritillner", n=3200, includeRts=F) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
jpeg("Maybrit Illner per Day.jpg", width = 1500)
ggplot(mentions_per_day, aes(created, n)) + geom_line()
dev.off()
Illner <- tweets_df
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_german$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
tweets_unnested <- inner_join(tweets_unnested, sentiws, by = "word")
tweets_df <- Illner
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_german$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
tweets_unnested <- inner_join(tweets_unnested, sentiws, by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Maybrit Illner.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Maybrit Illner Sentiment Evolution")+
scale_x_date(date_breaks = "4 day", date_labels = "%d %m")
dev.off()
#Wordcloud
jpeg("Maybrit Illner Wordcloud.jpg", width = 1500)
tweets_unnested %>%
#  inner_join(get_sentiments("bing")) %>%
count(word, neg_pos, sort = TRUE) %>%
acast(word ~ neg_pos, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
##Topic Analysis
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(Illner$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub("'", "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '???')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("german")) #when specifying function parameters, they are specified as arguments after the function
#otherStopWords <- c("this","the",".", "ca", "your", "just", "even", "new", "use", "right", "one", "today", "time", "pick", "here", "will", "give", "otherwhise", "can", "here", "part", "look", "part") #other stopwords, they were defined after inspecting the results without them. You can try to execute this line, analyze the results and check for words that provide no meaning. You will find all of those.
otherStopWords <- c("die","ein","das","der")
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "german")
#Wordcloud
jpeg("Maybritt Illner General Wordcloud.jpg", width = 1500)
wordcloud(tweetsCorpus, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
dev.off()
tweetsDTM <- DocumentTermMatrix(tweetsCorpus)
tweetsDTM
tweetsDTM <- tweetsDTM[unique(tweetsDTM$i), ] #pick only non zero-entry documents
tweetsDTM
topicNumberMetrics <- FindTopicsNumber(
tweetsDTM, #DTM object
topics = seq(from = 2, to = 15, by = 1), #Number of topics to test, i.e. between 2 and 15
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #Metrics to check
control = list(seed = 100), #Random seed
verbose = TRUE
)
FindTopicsNumber_plot(topicNumberMetrics)
tweetsLDA <- LDA(tweetsDTM, k = 3, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Maybritt Illner Topics.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
##Topic Analysis
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(Illner$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub("'", "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '???')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("deutsch")) #when specifying function parameters, they are specified as arguments after the function
#otherStopWords <- c("this","the",".", "ca", "your", "just", "even", "new", "use", "right", "one", "today", "time", "pick", "here", "will", "give", "otherwhise", "can", "here", "part", "look", "part") #other stopwords, they were defined after inspecting the results without them. You can try to execute this line, analyze the results and check for words that provide no meaning. You will find all of those.
otherStopWords <- c("die","ein","das","der")
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "deutsch")
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "german")
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("german")) #when specifying function parameters, they are specified as arguments after the function
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stop_german$word)
##Topic Analysis
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(Illner$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub("'", "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '???')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("german")) #when specifying function parameters, they are specified as arguments after the function
#otherStopWords <- c("this","the",".", "ca", "your", "just", "even", "new", "use", "right", "one", "today", "time", "pick", "here", "will", "give", "otherwhise", "can", "here", "part", "look", "part") #other stopwords, they were defined after inspecting the results without them. You can try to execute this line, analyze the results and check for words that provide no meaning. You will find all of those.
otherStopWords <- c("die","ein","das","der")
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stop_german$word)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "german")
#Wordcloud
jpeg("Maybritt Illner General Wordcloud.jpg", width = 1500)
wordcloud(tweetsCorpus, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
dev.off()
tweetsLDA <- LDA(tweetsDTM, k = 3, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Maybritt Illner Topics.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
##Topic Analysis
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(Heute$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub("'", "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '???')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("german")) #when specifying function parameters, they are specified as arguments after the function
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stop_german$word)
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "german")
#Wordcloud
jpeg("Heute Journal General Wordcloud.jpg", width = 1500)
wordcloud(tweetsCorpus, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
dev.off()
tweetsLDA <- LDA(tweetsDTM, k = 2, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Heute Journal Topics.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@jensspahn", n=3200, includeRts=F) # userTimline to get the n first tweets
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@jensspahn", n=3200, includeRts=T) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
jpeg("Jens Spahn per Day.jpg", width = 1500)
ggplot(mentions_per_day, aes(created, n)) + geom_line()
dev.off()
Spahn <- tweets_df
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_german$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
tweets_unnested <- inner_join(tweets_unnested, sentiws, by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Jens Spahn.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Jens Spahn Sentiment Evolution")+
scale_x_date(date_breaks = "4 day", date_labels = "%d %m")
dev.off()
#Wordcloud
jpeg("Jens Spahn Wordcloud.jpg", width = 1500)
tweets_unnested %>%
#  inner_join(get_sentiments("bing")) %>%
count(word, neg_pos, sort = TRUE) %>%
acast(word ~ neg_pos, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
##Topic Analysis
#' Extract the actual text and convert this corpus into a Term Document Matrix "borisjohnson"
#'
tweetsCorpus <- Corpus(VectorSource(Spahn$text))
tweetsCorpus
changeStrangeApostrophes <- content_transformer(function(x) {return (gsub("'", "'", x))})
tweetsCorpus <- tm_map(tweetsCorpus, changeStrangeApostrophes)
#Remove URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
tweetsCorpus <- tm_map(tweetsCorpus, removeURL)
#Remove dashes and quotation marks
remove <- content_transformer(function(x, pattern) {return (gsub(pattern, '', x))})
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '-')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '???')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "'")
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, '"')
tweetsCorpus <- tm_map(tweetsCorpus, remove, "/")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "@")
tweetsCorpus <- tm_map(tweetsCorpus, remove, "\\|")
#Remove Numbers
tweetsCorpus <- tm_map(tweetsCorpus, removeNumbers)
#Remove punctuation
tweetsCorpus <- tm_map(tweetsCorpus, removePunctuation)
#Stip whitespaces
tweetsCorpus <- tm_map(tweetsCorpus, stripWhitespace)
#Text to lowercase
#tweetsCorpus <- tm_map(tweetsCorpus, content_transformer(tolower))
#Remove stopwords
tweetsCorpus <-  tm_map(tweetsCorpus, removeWords, stopwords("german")) #when specifying function parameters, they are specified as arguments after the function
#otherStopWords <- c("this","the",".", "ca", "your", "just", "even", "new", "use", "right", "one", "today", "time", "pick", "here", "will", "give", "otherwhise", "can", "here", "part", "look", "part") #other stopwords, they were defined after inspecting the results without them. You can try to execute this line, analyze the results and check for words that provide no meaning. You will find all of those.
otherStopWords <- c("die","ein","das","der")
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, otherStopWords)
tweetsCorpus <- tm_map(tweetsCorpus, removeWords, stop_german$word)
#Stem the document
tweetsCorpus <- tm_map(tweetsCorpus, stemDocument, language = "german")
#Wordcloud
jpeg("Jens Spahn General Wordcloud.jpg", width = 1500)
wordcloud(tweetsCorpus, colors=brewer.pal(8,"Dark2"), max.words = 200, min.freq = 3)
dev.off()
tweetsDTM <- DocumentTermMatrix(tweetsCorpus)
tweetsDTM
tweetsDTM <- tweetsDTM[unique(tweetsDTM$i), ] #pick only non zero-entry documents
tweetsDTM
topicNumberMetrics <- FindTopicsNumber(
tweetsDTM, #DTM object
topics = seq(from = 2, to = 15, by = 1), #Number of topics to test, i.e. between 2 and 15
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), #Metrics to check
control = list(seed = 100), #Random seed
verbose = TRUE
)
FindTopicsNumber_plot(topicNumberMetrics)
tweetsLDA <- LDA(tweetsDTM, k = 3, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Jens Spahn Topics.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
##Country sentiment analysis
country_df <- rbind(Heute, Spahn, Illner)
DE <- country_df
#' Convert the initial data into Tidy Text Format
country_df$id <- seq(1, nrow(country_df))
tweets_unnested <- unnest_tokens(country_df, word, text)
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_german$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
tweets_unnested <- inner_join(tweets_unnested, sentiws, by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
country_df <- inner_join(country_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, country_df, mean)
#' Plot the average sentiment per date
jpeg("GE.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("GE Sentiment Evolution")+
scale_x_date(date_breaks = "4 day", date_labels = "%d %m")
dev.off()
#Wordcloud
jpeg("GE Wordcloud.jpg", width = 1500)
tweets_unnested %>%
#  inner_join(get_sentiments("bing")) %>%
count(word, neg_pos, sort = TRUE) %>%
acast(word ~ neg_pos, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
dev.off()
tweetsLDA <- LDA(tweetsDTM, k = 2, control = list(seed = 100))
tidyLda <- tidy(tweetsLDA)
topTerms <- tidyLda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
jpeg("Jens Spahn Topics.jpg", width = 1500)
topTerms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>% #previous code is mainly done to order the terms per topic according to their beta
ggplot(aes(term, beta, fill = as.factor(topic))) + # x axis to term  y to beta and fill them according to the topic number
geom_col(show.legend = FALSE) + #column graph without legend
coord_flip() + #flip graphs
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 10 terms in each LDA topic",
x = NULL, y = expression(beta)) +  #set title annd y label.
facet_wrap(~ topic, ncol = 3, scales = "free") #divide by topic
dev.off()
tweets <- get_timeline("@realDonaldTrump", n = 3200)
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=3200, includeRts=T) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
ggplot(mentions_per_day, aes(created, n)) + geom_line()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=3200, includeRts=T) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
ggplot(mentions_per_day, aes(created, n)) + geom_line()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=3200, includeRts=F) # userTimline to get the n first tweets
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=3200, includeRts=F) # userTimline to get the n first tweets
