scale_size_continuous(guide = FALSE, range = c(0.25, 2)) + # scale for edge widths
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat)),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
#scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
#geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
#hjust = 0, nudge_x = 1, nudge_y = 4,
#size = 1.5, color = "white", fontface = "bold") +
mapcoords + maptheme
nodes$weight = hub_score(g)
View(nodes)
#Plot with only countries size based on degree
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
nodes$weight = degree(g)
#Plot with only countries size based on degree
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
nodes$weight = hub.score(g)
nodes$weight = degree(g)
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1000, height ="1000")
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg")
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
?jpeg
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 300)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1000)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1500)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#Pot with most important connections (>5)
jpeg("MainConnections.jpg", width = 1500)
ggplot(nodes) + country_shapes +
geom_curve(aes(x = as.numeric(x), y = as.numeric(y), xend = as.numeric(xend), yend = as.numeric(yend),
size = weight),     # draw edges as arcs
data = important_edges, curvature = 0.33,
alpha = 0.5) +
scale_size_continuous(guide = FALSE, range = c(0.25, 2)) + # scale for edge widths
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat)),           # draw nodes
shape = 21, fill = 'white',
color = 'black', stroke = 0.5) +
mapcoords + maptheme
dev.off()
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1500)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'red',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 6)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1500)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'red',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 10)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#Plot with only countries size based on degree
jpeg("DegreeCentrality.jpg", width = 1500)
ggplot(nodes) + country_shapes +
geom_point(aes(x = as.numeric(lon), y = as.numeric(lat), size = weight),           # draw nodes
shape = 21, fill = 'red',
color = 'black', stroke = 0.5) +
scale_size_continuous(guide = FALSE, range = c(1, 8)) +    # scale for node size
geom_text(aes(x = as.numeric(lon), y = as.numeric(lat), label = name),             # draw text labels
hjust = 0, nudge_x = 1, nudge_y = 4,
size = 1.5, color = "black", fontface = "bold") +
mapcoords + maptheme
dev.off()
#'Setup
remove(list = ls())
#'Install Packages
library(dplyr) #' for sentiment analysis
library(tidytext) #' for sentiment analysis
library(ggplot2)
#install.packages("twitteR")
library(twitteR)
library(ldatuning)
library(topicmodels)
library(tm)
#'Login to twitter developer
options(httr_oauth_cache=T)
my_api_key             <- "DdLxsLaCKmd7WTJx478P0E10j"
my_api_secret          <- "NLrFfUgkfJVrs3UXoSSs4KGrzYFBu4BcjnpopIDM4az2EX3I4a"
my_access_token        <- "1252197116849672192-JkvDiU5dOe7WiLCn4BTXRPWQ3KpIoC"
my_access_token_secret <- "91oR1BrHclWQZZUygvZOeQZYb4Rml7zkvk4HtNnZtqYDU"
setup_twitter_oauth(my_api_key,my_api_secret,my_access_token,my_access_token_secret)
?userTimeline
#'
#'  Boris Johnson Twitter Data
#'
tweets_bojo <- userTimeline("BorisJohnson", n=1000) # userTimline to get the n first tweets
View(tweets_bojo)
tweets_df_bojo <- twListToDF(tweets_bojo) # convert it into a dataframe
View(tweets_bojo)
View(tweets_df_bojo)
#' Only keeping date
tweets_df_bojo$created <- as.Date(tweets_df_bojo$created, "%d/%m/%Y") # add this column to the dataset
?userTimeline
#'
#'  Boris Johnson Twitter Data
#'
tweets_bojo <- userTimeline("BorisJohnson", n=2000) # userTimline to get the n first tweets
tweets_df_bojo <- twListToDF(tweets_bojo) # convert it into a dataframe
#' Only keeping date
tweets_df_bojo$created <- as.Date(tweets_df_bojo$created, "%d/%m/%Y") # add this column to the dataset
View(tweets_bojo)
View(tweets_df_bojo)
#' Count the number of mentions per day
mentions_per_day_bojo <- count(tweets_df_bojo, created)
View(mentions_per_day_bojo)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_bojo, aes(Dates, n)) + geom_point()
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_bojo, aes(created, n)) + geom_point()
ggplot(mentions_per_day_bojo[mentions_per_day_bojo$Dates > "2020-05-01",], aes(Dates, n)) + geom_point() # adjust date
ggplot(mentions_per_day_bojo[mentions_per_day_bojo$created > "2020-05-01",], aes(Dates, n)) + geom_point() # adjust date
ggplot(mentions_per_day_bojo[mentions_per_day_bojo$created > "2020-05-01",], aes(created, n)) + geom_point() # adjust date
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_bojo, aes(created, n)) + geom_point()
ggplot(mentions_per_day_bojo[mentions_per_day_bojo$created > "2020-05-01",], aes(created, n)) + geom_point() # adjust date
library(stringr)
tweets_df_bojo$id <- seq(1, nrow(tweets_df_bojo))
tweets_unnested_bojo <- unnest_tokens(tweets_df_bojo, word, text)
View(tweets_unnested_bojo)
#' Remove stopwords and strange words
tweets_unnested_bojo <- tweets_unnested_bojo[!tweets_unnested_bojo$word %in% stop_words$word, ]
tweets_unnested_bojo <- tweets_unnested_bojo[str_detect(tweets_unnested_bojo$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested_bojo <- inner_join(tweets_unnested_bojo, get_sentiments("afinn"), by = "word")
View(tweets_unnested_bojo)
#' Compute average polarity per tweet
tweet_polarity_bojo <- aggregate(value ~ id, tweets_unnested_bojo, mean)
View(tweet_polarity_bojo)
tweets_df_bojo <- inner_join(tweets_df_bojo, tweet_polarity_bojo, by ="id")
#' Compute the average sentiment per date
date_sentiment_bojo <- aggregate(value ~ created, tweets_df_bojo, mean)
View(date_sentiment_bojo)
#' Plot the average sentiment per date
ggplot(date_sentiment_bojo, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Sentiment Corona Mentions")
#' Plot the average sentiment per date
jpeg("Boris Johnson.jpg", width = 1500)
ggplot(date_sentiment_bojo, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Boris Johnson Sentiment Evolution")
dev.off()
#' Wordcloud: to take a first look at the most frequency words and visualize it with a worldcloud
#'
m2 <- as.matrix(tweets_df_bojo)
v2 <- sort(rowSums(m2), decreasing=TRUE)
View(m2)
#'
#'  Emma Watson Twitter Data
#'
tweets_emma <- userTimeline("EmmaWatson", n=2000) # userTimline to get the n first tweets
tweets_df_emma <- twListToDF(tweets_emma) # convert it into a dataframe
#' Only keeping date
tweets_df_emma$created <- as.Date(tweets_df_emma$created, "%d/%m/%Y") # add this column to the dataset
View(tweets_unnested_bojo)
#' Count the number of mentions per day
mentions_per_day_emma <- count(tweets_df_emma, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_emma, aes(created, n)) + geom_point()
tweets_df_emma <- tweets_df_emma %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day_emma <- count(tweets_df_emma, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day_emma, aes(created, n)) + geom_point()
#'Setup
remove(list = ls())
#'Login to twitter developer
options(httr_oauth_cache=T)
my_api_key             <- "DdLxsLaCKmd7WTJx478P0E10j"
my_api_secret          <- "NLrFfUgkfJVrs3UXoSSs4KGrzYFBu4BcjnpopIDM4az2EX3I4a"
my_access_token        <- "1252197116849672192-JkvDiU5dOe7WiLCn4BTXRPWQ3KpIoC"
my_access_token_secret <- "91oR1BrHclWQZZUygvZOeQZYb4Rml7zkvk4HtNnZtqYDU"
setup_twitter_oauth(my_api_key,my_api_secret,my_access_token,my_access_token_secret)
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("BorisJohnson", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Boris Johnson.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Boris Johnson Sentiment Evolution")
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@guardian", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("The Guardian.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("The Guardian Sentiment Evolution")
dev.off()
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500) # userTimline to get the n first tweets
View(tweets)
tweets_df <- twListToDF(tweets) # convert it into a dataframe
View(tweets_df)
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
#'Setup
remove(list = ls())
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#'Login to twitter developer
options(httr_oauth_cache=T)
my_api_key             <- "CeAIf6W3DcwZ0gGe3UHihhZgZ"
my_api_secret          <- "muhNVlMH6uBVnCle1zdlmIdFkqIH94NMwOGhYMECamDzHWG4jw"
my_access_token        <- "1042754223258193921-S4KcAhqAkmie0zp9XIArY3hM6UTgXQ"
my_access_token_secret <- "5YCHFa6MoFhY3FcuKIJmQiJt7VFD0fHxnjwWxomzIDMZI"
setup_twitter_oauth(my_api_key,my_api_secret,my_access_token,my_access_token_secret)
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@realDonaldTrump", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
View(tweets_df)
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@TheEllenShow", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Ellen DeGeneres.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Ellen DeGeneres Sentiment Evolution")
dev.off()
#'Setup
remove(list = ls())
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@nytimes", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("NY Times.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("New York Times Sentiment Evolution")
dev.off()
#'Setup
remove(list = ls())
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@alain_berset", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Alain Berset.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Alain Berset Sentiment Evolution")
dev.off()
#'Setup
remove(list = ls())
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@rogerfederer", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
tweets_df$id <- seq(1, nrow(tweets_df))
tweets_unnested <- unnest_tokens(tweets_df, word, text)
#' Remove stopwords and strange words
tweets_unnested <- tweets_unnested[!tweets_unnested$word %in% stop_words$word, ]
tweets_unnested <- tweets_unnested[str_detect(tweets_unnested$word, "^[a-z']+$"),]
#' Pick token polarity from AFINN Lexicon
tweets_unnested <- inner_join(tweets_unnested, get_sentiments("afinn"), by = "word")
#' Compute average polarity per tweet
tweet_polarity <- aggregate(value ~ id, tweets_unnested, mean)
tweets_df <- inner_join(tweets_df, tweet_polarity, by ="id")
#' Compute the average sentiment per date
date_sentiment <- aggregate(value ~ created, tweets_df, mean)
#' Plot the average sentiment per date
jpeg("Roger Federer.jpg", width = 1500)
ggplot(date_sentiment, aes(x = created, y = value)) +
geom_smooth(method="loess", size = 1, se=T, span = .5) +
geom_hline(yintercept = 0, color = "grey") +
ylab("Avg. Sentiment") +
xlab("Date") +
ggtitle("Roger Federer Sentiment Evolution")
dev.off()
#'Setup
remove(list = ls())
#'
#'  X Twitter Data (generic name, just replace user in the first line and in the graph title)
#'
tweets <- userTimeline("@NZZ", n=2500) # userTimline to get the n first tweets
tweets_df <- twListToDF(tweets) # convert it into a dataframe
#' Only keeping date after February 2020
tweets_df$created <- as.Date(tweets_df$created, "%d/%m/%Y") # add this column to the dataset
tweets_df <- tweets_df %>% filter(created >="2020-02-01")
#' Count the number of mentions per day
mentions_per_day <- count(tweets_df, created)
#' Plot the number of mentions per day to see if the amount of tweet increased with the corona spread
ggplot(mentions_per_day, aes(created, n)) + geom_point()
